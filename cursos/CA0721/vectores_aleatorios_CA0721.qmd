---
title: "Vectores Aleatorios"
sidebar: ca0721
format: html
---

## Definición

Un vector aleatorio (_v.e_) sobre un e.p $(\Omega, \mathcal{F}, P)$ es una función

$$
X : \Omega \to \mathbb{R}^n \quad | \quad X^{-1}(B) \in \mathcal{F} \quad \forall B \in \mathcal{B}(\mathbb{R}^n)
$$

donde $\mathcal{B}(\mathbb{R}^n)$ denota la $\sigma$-álgebra de Borel en $\mathbb{R}^n$, es decir $\mathcal{B}(\mathbb{R}^n)$ es la $\sigma$-álgebra generada por todos los conjuntos abiertos de $\mathbb{R}^n$.

Para mantener la presentación sencilla se expondrá la teoría usando vectores $(X, Y)$ aunque la teoría es fácilmente extendible a vectores en más dimensiones.

---

## Función de distribución

La función de distribución conjunta de un vector aleatorio continuo $(X, Y)$ corresponde a la función

$$
F_{X,Y}(x, y) = P[X \leq x, Y \leq y]
$$

Se pueden recuperar las funciones de distribución mediante

$$
F_X(x) = \lim_{y \to +\infty} F_{X,Y}(x, y), \quad 
F_Y(y) = \lim_{x \to +\infty} F_{X,Y}(x, y)
$$

En el caso de vectores aleatorios absolutamente continuos existe una función no negativa $f_{X,Y}(x, y)$ tal que

$$
F_{X,Y}(x, y) = \int_{-\infty}^x \int_{-\infty}^y f_{X,Y}(u, v) \, dv \, du
$$

A $f_{X,Y}(x, y)$ se le dice la _función de densidad conjunta_ (_joint pdf_) de $(X, Y)$.

En particular, cuando $f_{X,Y}(x, y)$ es continua, se cumple que

$$
\frac{\partial^2}{\partial x \, \partial y} F_{X,Y}(x, y) = f_{X,Y}(x, y)
$$

---

## Densidades marginales

$f_{X,Y}(x, y)$ continua es la _joint pdf_ de $(X, Y)$. Se pueden encontrar las pdf $f_X, f_Y$ de la siguiente manera:

$$
\begin{aligned}
f_X(x) &= \frac{d}{dx} P(X \leq x) \\
       &= \frac{d}{dx} \int_{-\infty}^x \int_{-\infty}^{+\infty} f_{X,Y}(u, v) \, dv \, du \\
       &= \int_{-\infty}^{+\infty} f_{X,Y}(x, v) \, dv
\end{aligned}
$$

---

## Cálculo de probabilidades

Sea $D \subset \mathbb{R}^2$ una región en el plano que cumple con las condiciones para ser Riemann integrable.

$f_{X,Y}(x, y)$ es la _joint pdf_ de $(X, Y)$ y tiene la siguiente propiedad:

$$
P[(X, Y) \in D] = \iint_D f_{X,Y}(x, y) \, dA
$$

Además, bajo las condiciones del Teorema de Fubini se cumple que la anterior integral doble puede calcularse tanto como una integral iterada en el orden $dx \, dy$ como una en el orden $dy \, dx$, aunque para esto los límites de integración deben acomodarse adecuadamente.

---

## Independencia

Si $r, s$ son funciones continuas y $X, Y$ son independientes, entonces $r(X)$ y $s(Y)$ también son independientes.

A partir de dicha definición puede probarse que $X, Y$ son independientes:

$$
\Longleftrightarrow F_{X,Y}(x, y) = F_X(x) F_Y(y) \quad \forall x, y \in \mathbb{R}
$$

Asumiendo que $F_X, F_Y$ son derivables $\forall \mathbb{R}$, entonces la anterior igualdad ocurre:

$$
\Longleftrightarrow f_{X,Y}(x, y) = f_X(x) f_Y(y) \quad \forall x, y \in \mathbb{R}
$$

**Teorema de Factorización:**  
Si el vector $(X, Y)$ es absolutamente continuo, entonces

$$
f_{X,Y}(x, y) = g(x) h(y) \quad \forall x, y \in \mathbb{R}
$$

para ciertas funciones $g, h$.

## Densidad de una suma

::: callout-note
**Teorema:**  
Si $(X, Y)$ es un vector absolutamente continuo y $f_{X,Y}$ es su función de densidad conjunta, entonces la función de densidad de $X + Y$ viene dada por:

$$
f_{X+Y}(z) = \int_{-\infty}^{+\infty} f_{X,Y}(u, z - u) \, du \quad \text{para } z \in \mathbb{R}
$$

**Nota:**  
Como corolario del teorema anterior se obtiene que si $X, Y$ son v.a. absolutamente continuas e independientes, entonces $f_{X,Y}(u, z - u) = f_X(u) f_Y(z - u)$ y:

$$
f_{X+Y}(z) = \int_{-\infty}^{+\infty} f_X(u) f_Y(z - u) \, du \quad \text{para } z \in \mathbb{R}
$$
:::

---

## Transformaciones en vectores aleatorios

Sea $(X, Y)$ un v.e. abs. continuo con joint pdf $f_{X,Y}(x, y)$ y sean $g_1(x, y), g_2(x, y)$ dos funciones.  

Defínase:  

- $A = \{(x, y) \in \mathbb{R}^2 : f_{X,Y}(x, y) > 0\}$  
- $B = \{(u, v) \in \mathbb{R}^2 : u = g_1(x, y), v = g_2(x, y) \ \text{para algún } (x, y) \in A\}$

Suponga que $g_1, g_2$ son inyectivas en $A$ de modo que la transformación $(U, V) = T(x, y) = (g_1(x, y), g_2(x, y))$ es biyectiva como función de $A$ a $B$ y en particular $T^{-1}(u, v) = (x, y) = (h_1(u, v), h_2(u, v))$ existe.

Suponga que $h_1, h_2$ son $C^1$ en $B$ y que además para todo $(u, v) \in B$ se tiene que

$$
J := \left| \begin{array}{cc}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
\end{array} \right| \neq 0
$$

Entonces el vector aleatorio $(U, V)$ es absolutamente continuo y su densidad conjunta $f_{U,V}(u, v)$ viene dada por:

$$
f_{U,V}(u, v) = \begin{cases}
0 & \text{si } (u, v) \notin B \\
f_{X,Y}(x(u, v), y(u, v)) \cdot |J| & \text{si } (u, v) \in B
\end{cases}
$$

---

## Acerca de independencia

Los conceptos de independencia, el teorema de factorización, funciones generadoras de momentos y cambios de variable para variables aleatorias se extienden de manera natural a dimensiones mayores a 2.

---

## Esperanzas

Sea $(X, Y)$ un v.e con joint pdf $f_{X,Y}(x,y)$ y sea $g : \mathbb{R}^2 \to \mathbb{R}$ una función continua. Es importante señalar que incluso sin conocer dicha densidad el teorema del estadístico inconsciente sigue siendo válido en este contexto y nos dice que:

$$
\mathbb{E}[g(X,Y)] = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} g(x,y) f_{X,Y}(x,y) \, dy dx
$$

siempre que la integral sea absolutamente convergente.

---

### Propiedades

- Si $\mathbb{E}[X]$ y $\mathbb{E}[Y]$ existen, entonces:

$$
\mathbb{E}[\alpha X + \beta Y] = \alpha \mathbb{E}[X] + \beta \mathbb{E}[Y] \quad \text{para todo } \alpha, \beta \in \mathbb{R}
$$

- Si $X, Y$ son variables independientes, entonces:

$$
\mathbb{E}[XY] = \mathbb{E}[X] \, \mathbb{E}[Y]
$$

- Si $X \geq 0$ c.s., entonces $\mathbb{E}[X] \geq 0$.

- El anterior resultado puede usarse para deducir algunas desigualdades tales como[^5]:

  1. **Monotonicidad de la esperanza:**  
     Si $X \leq Y$ c.s., entonces  
     
     $$\mathbb{E}[X] \leq \mathbb{E}[Y]\qquad\text{(cuando ambas esperanzas existan)}$$

  2. **Desigualdad triangular:**  
     
     $$|\mathbb{E}[X]| \leq \mathbb{E}[|X|] \quad \text{ cuando }\quad\mathbb{E}(|X|) < \infty$$ 

  3. **Cauchy-Schwarz:**  
     
     $$|\mathbb{E}[XY]| \leq \sqrt{\mathbb{E}(X^2)} \sqrt{\mathbb{E}(Y^2)}\quad \text{ cuando }\quad \mathbb{E}[X^2] < \infty \; \text{ y }\; \mathbb{E}[Y^2] < \infty$$

---

## Covarianza, correlación y matriz de covarianza

Denótese:  
$$\mu_X = \mathbb{E}(X) \qquad \mu_Y = \mathbb{E}(Y)$$

- El valor esperado del vector $(X, Y)$ es el vector $(\mu_X, \mu_Y)$.

- La **covarianza** de las variables $X, Y$ se define como:

$$
\text{Cov}[X, Y] = \mathbb{E} \left[ (X - \mu_X)(Y - \mu_Y) \right]
$$

(De donde se deduce):

$$
\text{Var}[X] = \text{Cov}[X, X], \quad \text{Cov}[X, Y] = \text{Cov}[Y, X]
$$

- Si $\text{Var}[X]$ y $\text{Var}[Y]$ existen, entonces:

$$
\text{Var}[\alpha X + \beta Y] = \alpha^2 \text{Var}[X] + \beta^2 \text{Var}[Y] + 2\alpha \beta \text{Cov}[X, Y]
$$

- En particular, si $X$ y $Y$ son independientes, entonces:

$$
\text{Var}[\alpha X + \beta Y] = \alpha^2 \text{Var}[X] + \beta^2 \text{Var}[Y]
$$

- La **matriz de covarianza** del vector $(X, Y)$ es la matriz:

$$
\Sigma(X,Y) =
\begin{pmatrix}
\text{Var}[X] & \text{Cov}[X,Y] \\
\text{Cov}[Y,X] & \text{Var}[Y]
\end{pmatrix}
$$

- El **coeficiente de correlación** entre $X$ y $Y$ se define como:

$$
\rho_{XY} = \frac{\text{Cov}[X,Y]}{\sigma_X \sigma_Y}
$$

- La **matriz de correlación** del vector $(X, Y)$ es la matriz[^6]:

$$
\begin{pmatrix}
1 & \rho_{XY} \\
\rho_{YX} & 1
\end{pmatrix}
$$

---

### Otras propiedades

Sean $X, Y$ dos variables aleatorias:

- Sea $X$ una v.a. no negativa c.s. Entonces $\mathbb{E}[X] = 0$ si y solo si $X = 0$ c.s.

- $\text{Cov}[X, Y] = \mathbb{E}[XY] - \mu_X \mu_Y$

- Si $X, Y$ son independientes, entonces $\text{Cov}[X, Y] = 0$

- $|\text{Cov}[X,Y]| \leq \sqrt{\text{Var}[X]} \sqrt{\text{Var}[Y]}$.  
  En particular, se tiene que $-1 \leq \rho_{XY} \leq 1$

[^5]: Estas propiedades son válidas para todos los tipos de variables aleatorias (discretas, absolutamente continuas, mixtas, etc).  
[^6]: Tanto la matriz de covarianza como la de correlación son matrices simétricas.

---

## Función generadora de momentos

La función generadora de momentos de un vector $X = (X_1, X_2, \dots, X_n)$ corresponde a la función:

$$
M_X(t_1, t_2, \dots, t_n) = \mathbb{E} \left[ e^{\sum\limits_{j=1}^n t_j X_j} \right]
$$

definida en aquella región de $\mathbb{R}^n$ donde dicha esperanza exista.

::: callout-note
**Nota 1:**  
En el caso de que $(X_1, X_2, \dots, X_n)$ sean independientes, se cumple que

$$
M_X(t_1, t_2, \dots, t_n) = \prod_{j=1}^n M_{X_j}(t_j)
$$

**Nota 2:**  
Similar al caso de dimensión 1, si la anterior $M_X(t_1, t_2, \dots, t_n)$ existe en un intervalo abierto del origen $\vec{0}$, entonces dicha función puede utilizarse para calcular momentos. Por ejemplo:

$$
\mathbb{E}[X_1^3] = \left. \frac{\partial^3}{\partial t_1^3} M_X(t_1, t_2, \dots, t_n) \right|_{t = \vec{0}}
$$
:::

---

## Distribución normal multivariada

Sea $\mu \in \mathbb{R}^n$, $\Sigma \in M(n, \mathbb{R})$ donde $\Sigma^T = \Sigma$ y $\Sigma$ es una matriz definida positiva. Se dice que el v.e. $X = (X_1, X_2, \dots, X_n)$ tiene una distribución normal multivariada con media $\mu$ y matriz de covarianza $\Sigma$, si su función de densidad viene dada por:

$$
f(X) = \frac{1}{(2\pi)^{n/2} |\Sigma|^{1/2}} \cdot e^{-\frac{1}{2} (X - \mu)^T \Sigma^{-1} (X - \mu)}
$$

Al igual que en el caso de la normal bivariada, si el vector $X$ es normal multivariado entonces cada componente del vector es normal. De hecho, usando el resultado de la diapositiva anterior puede probarse que toda combinación lineal de las variables $X_1, X_2, \dots, X_n$ es normal también.

::: callout-warning
**Cuidado:**  
Como vimos anteriormente puede ocurrir que $X, Y$ sean dos variables normales (no independientes), pero que el vector $(X, Y)$ no sea normal bivariado. Esta situación se generaliza para dimensiones superiores.
:::

## Vectores Aleatorios Discretos

Lo de vectores aleatorios absolutamente continuos puede ser adaptado para ser utilizado en el caso de vectores discretos.

### Función de masa condicional

Sea $(X, Y)$ un v.e. discreto con función de probabilidad $p_{X,Y}(x,y)$ y marginales $p_X(x)$, $p_Y(y)$. $\forall x : p_X(x) > 0$.

Se define la pmf condicional de $Y$ dado que $X = x$ como:

$$
p_{Y|X}(y \mid x) = \frac{P(X = x, Y = y)}{P(X = x)} = \frac{p(x,y)}{p_X(x)}
$$

Similarmente, para cada $y$ tal que $p_Y(y) > 0$ se define la pmf de $X$ dado que $Y = y$ como:

$$
p_{X|Y}(x \mid y) = \frac{p_{X,Y}(x,y)}{p_Y(y)}
$$

---

### Esperanza condicional

Sea $Y$ una v.a. tal que $\mathbb{E}|Y| < \infty$ y sea $g$ una función.  
La esperanza condicional de $g(Y)$ dado $X = x$ se define como:

$$
\phi(x) := \mathbb{E}(g(Y) \mid X = x) = \sum_{y \in \text{Im}(Y)} g(y) \, p_{Y|X}(y \mid x)
$$

siempre que la anterior suma (en caso de ser una serie) converja absolutamente.

---

## Densidad condicional

Sea $(X,Y)$ un v.e. absolutamente continuo con joint pdf $f_{X,Y}(x,y)$  
y con marginal pdf $f_X(x)$, $f_Y(y)$.  
Para cada $x$ tal que $f_X(x) > 0$ se define la pdf condicional de $Y$ dado que $X = x$ como:

$$
f_{Y|X}(y \mid x) = \frac{f_{X,Y}(x,y)}{f_X(x)}
$$

Similarmente $\forall y : f_Y(y) > 0$ se define la densidad condicional de $X$ dado que $Y = y$ como:

$$
\phi(x) := \mathbb{E}[g(Y) \mid X = x] = \int_{-\infty}^{+\infty} g(y) \cdot f_{Y|X}(y|x) \, dy
$$

siempre que la anterior integral converja absolutamente.  
De manera similar al caso discreto, se define $\mathbb{E}[g(X) \mid X]$.

---

## Varianza condicional

También, la varianza condicional de $Y$ dada $X = x$ se define como:

$$
r(x) = \text{Var}[Y \mid X = x] = \mathbb{E}\left[Y^2 \mid X = x\right] - \left( \mathbb{E}[Y \mid X = x] \right)^2
$$

Esta definición aplica tanto para el caso discreto como para el caso continuo.

### Propiedades

Tanto para el caso discreto como absolutamente continuo (y otros más generales), se cumplen las siguientes propiedades (asumiendo que las esperanzas y varianzas involucradas existen):

- **Ley de esperanza total:**

  La esperanza condicional $\mathbb{E}[X \mid Y]$ es una v.a. la cual es una función de $Y$ y satisface que:

  $$
  \mathbb{E}[\mathbb{E}[X \mid Y]] = \mathbb{E}[X]
  $$

  Más generalmente:

  $$
  \mathbb{E}[X] = \sum_{i=1}^n P(A_i) \mathbb{E}[X \mid A_i]
  $$

- **Linealidad:**

  La esperanza condicional es lineal, i.e.:

  $$
  \mathbb{E}[\alpha X + \beta Y \mid Z] = \alpha \cdot \mathbb{E}[X \mid Z] + \beta \cdot \mathbb{E}[Y \mid Z]
  $$

- **Independencia:**

  Si $X, Y$ son independientes entonces:

  $$
  \mathbb{E}[X \mid Y] = \mathbb{E}[X]
  $$

- **Sacar información conocida:**

  $$
  \mathbb{E}[X \mid X] = X
  $$

  $$
  \mathbb{E}[g(X) \mid X] = g(X)
  $$

  Y más generalmente:

  $$
  \mathbb{E}[X \cdot g(Y) \mid Y] = g(Y) \cdot \mathbb{E}[X \mid Y]
  $$

- **Ley de varianza total:**

  $$
  \text{Var}[X] = \mathbb{E}[\text{Var}[X \mid Y]] + \text{Var}(\mathbb{E}[X \mid Y])
  $$
  
## Definición formal de $\mathbb{E}[X \mid \mathcal{H}]$

Sea $(\Omega, \mathcal{F}, P)$ e.p y sea $X : \Omega \to \mathbb{R}$ una v.a. tal que $\mathbb{E}|X| < \infty$ y sea $\mathcal{H} \subset \mathcal{F}$ otra $\sigma$–álgebra.

Una esperanza condicional de $X$ dado $\mathcal{H}$ es cualquier v.a. $Z : \Omega \to \mathbb{R}$ tal que:

1. $Z^{-1}(B) \in \mathcal{H}$ para todo $B \in \mathcal{B}(\mathbb{R})$  
2. $\mathbb{E}[Z \cdot 1_H] = \mathbb{E}[X \cdot 1_H]$ para todo $H \in \mathcal{H}$

### Observaciones:

- La esperanza condicional no es la única que dicha unicidad sea entendida en un sentido casi siempre.
- La “única” esperanza condicional de $X$ dado $\mathcal{H}$ se denota por $\mathbb{E}[Y \mid \mathcal{H}]$.
- Cuando $Y$ es una v.a. se define $\mathbb{E}[X \mid Y]$ como $\mathbb{E}[X \mid \sigma(Y)]$ donde $\sigma(Y)$ es la $\sigma$–álgebra generada por la v.a. $Y$.
- La esperanza condicional $\mathbb{E}[X \mid \mathcal{H}]$ se interpreta como la mejor estimación que puede hacerse de $X$ en términos de la información contenida en $\mathcal{H}$.

---

## Esperanza condicional respecto a un evento

Sea $X$ en $(\Omega, \mathcal{F}, P)$ y sea $B \in \mathcal{F}$ con $P(B) > 0$.  
Suponga que $\mathbb{E}|X| < \infty$.

$$
\mathbb{E}[X \mid B] = \frac{\mathbb{E}[X \cdot \mathbf{1}_B]}{P(B)}
$$

Sean $(B_i)_{i=1}^n$ eventos en $(\Omega, \mathcal{F}, P)$ partición de $\Omega$:

$$
\mathbb{E}[X] = \sum_{i=1}^n \mathbb{E}[X \mid B_i] \cdot P(B_i)
$$

---

## Propiedad minimizadora de la esperanza

La esperanza condicional $\mathbb{E}[Y \mid X]$ suele ser interpretada como la mejor predicción que puede hacerse de la variable $Y$ a partir de la variable $X$.

Suponga que se observa el valor de una v.a. $X$ y luego, con base en el valor observado, se intenta predecir el valor de una segunda v.a. $Y$.

Denótese por $g(X)$ el predictor; i.e., si se observa que $X = x$, entonces $g(x)$ es nuestra predicción para el valor de $Y$.  
La intención es escoger $g(X)$ de manera que en cierta forma esté cerca de la variable $Y$.

Por supuesto, en general, hay distintas maneras en que se puede medir la cercanía de $g(X)$ con $Y$, pero pensemos que se desea utilizar como criterio la expresión:

$$
\mathbb{E}[(Y - g(X))^2]
$$

Para esto se requiere asumir que $Y$ y $g(X)$ satisfacen ciertas condiciones en relación con sus segundos momentos.

Asumiendo que tales condiciones se cumplen, no es difícil mostrar que:

$$
\mathbb{E}[(Y - g(X))^2] \geq \mathbb{E}[(Y - \mathbb{E}[Y \mid X])^2]
$$

De manera que, bajo esta noción de cercanía, se puede ver que $\mathbb{E}[Y \mid X]$ es en efecto el mejor predictor de $Y$ que puede hacerse a partir de $X$.

---

## Revisando la normal bivariada

Sean $Z_1, Z_2$ dos v.a. indep. con distribución $\mathcal{N}(0,1)$ y sean $\mu_X, \mu_Y \in \mathbb{R}, \sigma_X, \sigma_Y \in \mathbb{R}^+, \rho \in [-1,1]$.

Usando la fórmula de cambio de variable se puede mostrar que si  
$X = \sigma_X Z_1 + \mu_X$,  
$Y = \mu_Y + \sigma_Y(\rho Z_1 + \sqrt{1 - \rho^2} Z_2)$,  
entonces el vector $(X,Y)$ sigue una distribución normal bivariada donde:

- $\mathbb{E}(X) = \mu_X$  
- $\mathbb{E}(Y) = \mu_Y$  
- $\text{Var}(X) = \sigma_X^2$  
- $\text{Var}(Y) = \sigma_Y^2$  
- $\text{Cov}(X,Y) = \sigma_X \sigma_Y \rho$

De hecho, la anterior implicación es reversible en el sentido de que si el vector $(X,Y)$ sigue una distribución normal bivariada con $\mathbb{E}(X) = \mu_X$, $\mathbb{E}(Y) = \mu_Y$, $\text{Var}(X) = \sigma_X^2$, $\text{Var}(Y) = \sigma_Y^2$, $\text{Cov}(X,Y) = \sigma_X \sigma_Y \rho$, entonces existen variables aleatorias $Z_1, Z_2$ independientes con distribución $\mathcal{N}(0,1)$, tal que:

$$
X = \sigma_X Z_1 + \mu_X,\quad Y = \mu_Y + \sigma_Y (\rho Z_1 + \sqrt{1 - \rho^2} Z_2)
$$

Algo muy conveniente de los ejemplos es que permite usar propiedades de esperanzas, varianzas y covarianzas para verificar que:

- $\mathbb{E}(X) = \mu_X$
- $\mathbb{E}(Y) = \mu_Y$
- $\text{Var}(X) = \sigma_X^2$
- $\text{Var}(Y) = \sigma_Y^2$
- $\text{Cov}(X,Y) = \sigma_X \sigma_Y \rho$

Similarmente, usando propiedades de las variables con distribución normal y de la esperanza condicional, puede verse que si $(X,Y)$ es un v.e. con distribución normal bivariada con parámetros $\mu_X, \mu_Y, \sigma_X, \sigma_Y, \rho$, entonces:

$$
\mathbb{E}[Y \mid X = x] = \mu_Y + \rho \sigma_Y \frac{x - \mu_X}{\sigma_X}
$$

$$
\text{Var}[Y \mid X = x] = (1 - \rho^2) \sigma_Y^2
$$

**Nota:** Un resultado similar aplica para $X \mid Y = y$.

## Funciones importantes

Sea $\mathbf{X} = (X_1, \dots, X_n)$ un vector aleatorio $n$–dimensional. Se utiliza $\mathbf{t} \cdot \mathbf{X} = \mathbf{t}^\mathsf{T} \mathbf{X}$ en lugar de $tX$:

::: {.callout-note title="Función Característica"}

$$
\varphi_{\mathbf{X}}(\mathbf{t}) = \mathbb{E} \left[ e^{i \mathbf{t}^\mathsf{T} \mathbf{X}} \right] 
= \int_{\mathbb{R}^n} e^{i \mathbf{t}^\mathsf{T} \mathbf{x}} \cdot f_{\mathbf{X}}(\mathbf{x}) \, d\mathbf{x}
$$

Está definida para todo $\mathbf{t} \in \mathbb{R}^n$.
:::

::: {.callout-note title="Función Generadora de Momentos"}

$$
M_{\mathbf{X}}(\mathbf{t}) = \mathbb{E} \left[ e^{\mathbf{t}^\mathsf{T} \mathbf{X}} \right] 
= \int_{\mathbb{R}^n} e^{\mathbf{t}^\mathsf{T} \mathbf{x}} \cdot f_{\mathbf{X}}(\mathbf{x}) \, d\mathbf{x}
$$

Válida para todo $\mathbf{t}$ donde dicho valor esperado exista.
:::

::: {.callout-note title="Función Generadora de Probabilidades"}

$$
G_{\mathbf{X}}(\mathbf{t}) = \mathbb{E} \left[ \mathbf{t}^{\mathbf{X}} \right] 
= \int_{\mathbb{R}^n} \mathbf{t}^{\mathbf{x}} \cdot f_{\mathbf{X}}(\mathbf{x}) \, d\mathbf{x}
$$

Válida para todo $\mathbf{t}$ donde dicho valor esperado exista.
:::

## Estadísticos de orden

Sean $X_1, X_2, \dots, X_n$ variables independientes absolutamente continuas e idénticamente distribuidas.

Es claro que por definición:

$$
X_{(1)} \leq X_{(2)} \leq X_{(3)} \leq \cdots \leq X_{(n)}
$$

Definamos:

- $X_{(1)}$ = el más pequeño de $X_1, X_2, \dots, X_n$
- $X_{(2)}$ = el segundo más pequeño
- $X_{(3)}$ = el tercero más pequeño  
 $\;\;\vdots$
- $X_{(n)}$ = el mayor

Sean $f$ y $F$ la función de densidad y distribución (común) de las variables $X_1, X_2, \dots, X_n$, y $X_{(1)}, X_{(2)}, \dots, X_{(n)}$ los estadísticos de orden. Sean $i, j \in \{1, 2, \dots, n\}$:

### Función de densidad conjunta

$$
f_{X_{(1)}, \dots, X_{(n)}}(x_1, \dots, x_n) =
\begin{cases}
n! \prod\limits_{j=1}^{n} f(x_j) & \text{si } x_1 < x_2 < \cdots < x_n \\
0 & \text{en otro caso}
\end{cases}
$$

### Función de densidad del $j$-ésimo orden

$$
f_{X_{(j)}}(x) = \binom{n-1}{j-1} F^{j-1}(x) \cdot (1 - F(x))^{n-j} \cdot f(x)
$$

### Función de distribución de $X_{(j)}$

$$
F_{X_{(j)}}(x) = \sum_{k=j}^{n} \binom{n}{k} F^k(x)(1 - F(x))^{n-k}
$$

> _Intuición_: $X_{(j)} < x \Leftrightarrow$ al menos $j$ de las variables son menores que $x$

### Función de distribución conjunta de $X_{(i)}, X_{(j)}$ con $i < j$

$$
f_{X_{(i)}, X_{(j)}}(x_i, x_j) = 
\tbinom{n}{i-1, j-i-1, 1, n-j} F^{i-1}(x_i) (F(x_j) - F(x_i))^{j-i-1} (1 - F(x_j))^{n-j} f(x_i) f(x_j)
$$

> _Intuición_: Una variable igual a $x_i$, otra a $x_j$, $i-1$ menores a $x_i$, $n-j$ mayores que $x_j$, y $j-i-1$ entre $x_i$ y $x_j$.

---

### Rango de la muestra aleatoria

Sean $X_1, X_2, \dots, X_n$ v.a. independientes absolutamente continuas e idénticamente distribuidas con distribución uniforme $U(0,1)$.

Se define la variable del rango como:

$$
V = X_{(n)} - X_{(1)} \sim \text{Beta}(n - 1, 2)
$$